# Baldur Model Inference

This directory contains code for running inference with language models on formal proof datasets.

## Inference Script

The `inference.py` script allows you to run a language model on examples from a JSONL dataset file in the `eval` directory.

### Usage

```bash
python inference.py --base_model BASE_MODEL --dataset DATASET_NAME [OPTIONS]
```

### Required Arguments

- `--base_model`: The base model to use, must be one of: 'llemma', 'deepseek'
- `--dataset`: The dataset to use, must be one of: 'minilang', 'minilang-no-SH', 'isar', 'isar-SH'

### Optional Arguments

- `--output`: Output file to save results (defaults to results_{base_model}_{dataset}.jsonl)
- `--max_samples`: Maximum number of samples to process (default: all)
- `--max_new_tokens`: Maximum number of new tokens to generate (default: 2048)
- `--temperature`: Temperature for sampling (default: 0.0)
- `--top_p`: Top-p sampling parameter (default: 0.9)
- `--seed`: Random seed for reproducibility (default: 42)
- `--custom_model`: Override with a custom model name/path (for advanced users)
- `--batch_size`: Batch size for processing examples (default: 1)

### Batch Processing

The script supports batch processing for more efficient evaluation. By setting the `--batch_size` parameter, you can process multiple examples at once, which can significantly improve performance on GPU devices. The default batch size is 1 (no batching).

For example, to process examples in batches of 4:
```bash
python inference.py --base_model llemma --dataset minilang --batch_size 4
```

If an error occurs while processing a batch, the script will attempt to process the examples in that batch individually to avoid losing all results from that batch.

### Prompt Format

The script uses a fixed prompt format for all examples:

```
<s> You are an AI programming assistant, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.
</s> 
<s> user
PRELUDE:
{prelude}
GOAL:
{goal}</s>
<s> assistant
```

Where `{prelude}` and `{goal}` are the corresponding fields from the dataset.

### How Model Names Are Constructed

The script automatically constructs the full model name using the format:

```
haonan-li/{dataset_base}-{base_model}-7b
```

Where:
- `{dataset_base}` is the base name of the dataset (e.g., 'minilang' for both 'minilang' and 'minilang-no-SH')
- `{base_model}` is the specified base model (e.g., 'llemma' or 'deepseek')

For example:
- With `--base_model llemma --dataset minilang`, the script will use "haonan-li/minilang-llemma-7b"
- With `--base_model deepseek --dataset isar`, the script will use "haonan-li/isar-deepseek-7b"

### Example Usage

```bash
# Run inference with the llemma model on the first 5 examples from the minilang dataset
python inference.py --base_model llemma --dataset minilang --max_samples 5

# Run inference with the deepseek model on the isar dataset with custom output file
python inference.py --base_model deepseek --dataset isar --output results_deepseek_isar.jsonl --temperature 0.7

# Use a custom model instead of the automatically constructed model name
python inference.py --base_model llemma --dataset minilang --custom_model "meta-llama/Llama-2-7b-hf"

# Process examples in batches of 8 for faster evaluation
python inference.py --base_model llemma --dataset minilang --batch_size 8
```

### Output Format

The script generates a JSONL file where each line contains a JSON object with the following fields:

- `id`: The index of the example
- `prompt`: The formatted prompt given to the model
- `response`: The model's response (without the prompt)
- `full_generated_text`: The complete text generated by the model (includes the prompt)
- `model`: The full model name/path used
- `base_model`: The base model specified (llemma or deepseek)
- `temperature`: The temperature used for generation
- `top_p`: The top-p value used for generation
- `max_new_tokens`: The maximum number of tokens generated
- `prelude`: The prelude from the original example
- `goal`: The goal from the original example
- `dataset`: The dataset used for inference

## Available Datasets

The script supports the following datasets, stored as JSONL files in the `eval` directory:

- `minilang`: Examples in the MinILang format (`eval_data_minilang.jsonl`)
- `minilang-no-SH`: MinILang examples without Structural Hints (`eval_data_minilang-no-SH.jsonl`)
- `isar`: Examples in the Isar format (`eval_data_isar.jsonl`)
- `isar-SH`: Isar examples with structural hints (`eval_data_isar-SH*.jsonl`)

Each JSONL file contains examples with at least 'prelude' and 'goal' fields that are used to construct the prompts. 